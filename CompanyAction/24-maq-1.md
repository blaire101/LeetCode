## Data Engineer Interview Questions with Solutions

---

### **1. Identify Customers with Average Spend Per Day > 3**

#### **Problem Statement:**
We are given a table `transaction` with fields:  
- `customer_id` (string) - ID of the customer  
- `txn_date` (date) - Date of the transaction  
- `txn_time` (string) - Time of the transaction  
- `txn_amount` (int) - Transaction amount  

The task is to identify customers whose average daily transaction amount is greater than 3.

---

#### **Approach:**
1. Group transactions by `customer_id` and `txn_date` to calculate the total transaction amount for each customer on each day.
2. Calculate the average daily transaction amount by grouping by `customer_id` again.
3. Filter out customers whose average daily spend is greater than 3.

---

```sql
-- Step 1: Calculate daily transaction amount
WITH daily_spend AS (
    SELECT 
        customer_id, 
        txn_date,
        SUM(txn_amount) AS sum_spend
    FROM transaction
    GROUP BY customer_id, txn_date
)

-- Step 2: Calculate average daily spend and filter
SELECT 
    customer_id, 
    AVG(sum_spend) AS average_spend
FROM daily_spend
GROUP BY customer_id
HAVING AVG(sum_spend) > 3;
```

---

### **2. Calculate Average Days Between Transactions for Each Customer**

#### **Problem Statement:**
Given the same `transaction` table, calculate the average number of days between transactions for each customer.

---

#### **Approach:**
1. Use the `LAG` window function to get the previous transaction date for each customer.
2. Calculate the difference in days between the current and previous transactions using `DATEDIFF`.
3. Group by `customer_id` and calculate the average.

---

```sql
-- Step 1: Get previous transaction date
WITH transaction_date AS (
    SELECT 
        customer_id, 
        txn_date,
        LAG(txn_date) OVER (PARTITION BY customer_id ORDER BY txn_date) AS prev_transaction_date
    FROM transaction
)

-- Step 2: Calculate days between transactions
, date_diff AS (
    SELECT 
        customer_id, 
        DATEDIFF(txn_date, prev_transaction_date) AS date_between_transactions
    FROM transaction_date
    WHERE prev_transaction_date IS NOT NULL
)

-- Step 3: Calculate average days between transactions
SELECT 
    customer_id,
    AVG(date_between_transactions) AS avg_days_transactions
FROM date_diff
GROUP BY customer_id;
```

---

### **3. Find Tuples Matching a Sum Condition in Python**

#### **Problem Statement:**
Given four lists `A`, `B`, `C`, and `D`, find the number of tuples `(i, j, k, l)` such that:

\[
A[i] + B[j] + C[k] = D[l]
\]

---

#### **Approach:**
1. Precompute the sums of pairs from lists `A` and `B`, storing them in a dictionary `sum_AB`.
2. Iterate through pairs from lists `C` and `D`, checking if `D[l] - C[k]` exists in `sum_AB`.
3. Count matches based on occurrences stored in `sum_AB`.

---

```python
# Example lists
A = [1, 2, 3, 4]
B = [2, 4, 5, 6]
C = [1, 3, 4, 9]
D = [7, 4, 9, 10]

# Precompute sums from A and B
sum_AB = {}
for i in range(len(A)):
    for j in range(len(B)):
        s = A[i] + B[j]
        if s not in sum_AB:
            sum_AB[s] = 0
        sum_AB[s] += 1

# Count matching tuples
count = 0
for i in range(len(C)):
    for j in range(len(D)):
        target = D[j] - C[i]
        if target in sum_AB:
            count += sum_AB[target]

print(f"Number of tuples: {count}")
```

### 4. PySpark UDF for Column Matching Check

#### **Problem Statement:**
Given a PySpark DataFrame `df` with a column `before_checking`, use a UDF to check if each value exists in a predefined list `common_words`. Add a new column `after_checking` that contains `1` if the word exists and `0` otherwise.

---

#### **Approach:**
1. Define a Python function `check` that checks if a word exists in `common_words`.
2. Register this function as a PySpark UDF.
3. Use `withColumn` to create a new column `after_checking` based on the UDF.

---

```python
# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

# Initialize Spark session
spark = SparkSession.builder.appName("UDF Example").getOrCreate()

# Sample DataFrame
data = [("hello",), ("world",), ("example",), ("common",)]
df = spark.createDataFrame(data, ["before_checking"])

# Define function for matching check
def check(col, common_words):
    if col in common_words:
        return 1
    else:
        return 0

# Example list of common words
common_words = ["hello", "common"]

# Register UDF
check_udf = udf(lambda col: check(col, common_words), IntegerType())

# Apply UDF to DataFrame
df = df.withColumn("after_checking", check_udf(df["before_checking"]))

# Show result
df.show()
```